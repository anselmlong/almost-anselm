"""Serve the model for inference.

Placeholder for running an inference server (vLLM / HF / Ollama).
"""

def start_server(model_path: str = None, port: int = 8080):
    print(f"Starting inference server (placeholder) on port {port} with model {model_path}")


if __name__ == "__main__":
    start_server()
