base_model: mistralai/Mistral-7B-v0.1
model_type: mistral
tokenizer: mistralai/mistral-7b-v0.1

trust_remote_code: true
load_in_4bit: true
torch_dtype: float16

adapter: lora

datasets:
  - path: data/processed/sft_train_new.jsonl
    type: chat
    chat_template: chatml
    train: true
  - path: data/processed/sft_val_new.jsonl
    type: chat
    chat_template: chatml 
    validation: true

tokenizer:
  add_eos_token: true
  add_bos_token: true

sequence_length: 768
max_seq_length: 768
cutoff_len: 768
sample_packing: true

training:
  output_dir: models/base
  num_epochs: 1

  micro_batch_size: 1
  batch_size: 16
  gradient_accumulation_steps: 16

  learning_rate: 1e-4
  lr_scheduler: cosine
  warmup_steps: 100
  logging_steps: 50
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  fp16: true
  gradient_checkpointing: true
  optim: adamw_bnb_8bit
  max_grad_norm: 1.0


  peft:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj

  bnb_4bit_quantization_config:
    bnb_4bit_compute_dtype: float16
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4

