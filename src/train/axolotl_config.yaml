base_model: mistralai/Mistral-7B-v0.1
model_type: mistral

trust_remote_code: true
load_in_4bit: true
torch_dtype: float16

dataset:
  train: data/processed/sft_train.json
  val: data/processed/sft_val.json
  test: data/processed/sft_test.json
dataset_format: chatml

tokenizer:
  add_eos_token: true
  add_bos_token: true

sequence_length: 2048
sample_packing: true

adapter: lora

peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

bnb_4bit_quantization_config:
  bnb_4bit_compute_dtype: float16
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: nf4

training:
  output_dir: models/mistral-telegram
  num_train_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 1e-4
  lr_scheduler_type: cosine
  warmup_steps: 100
  logging_steps: 50
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3

  fp16: true
  bf16: false
  gradient_checkpointing: true
  optim: paged_adamw_32bit
