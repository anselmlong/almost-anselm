# ğŸ“Œ Project TODO â€” Telegram Persona LLM (â€œAnselm AIâ€)

This checklist tracks the full pipeline from Telegram export â†’ persona fine-tuning â†’ bot â†’ preference RL reinforcement.

---

## âœ… Phase 0 â€” Repo & Environment

- [X] Clone repository + set up folder structure
- [X] Create Python virtual env (`python -m venv venv`)
- [X] Install dependencies (`pip install -r requirements.txt`)
- [X] Fill in `.env` using `.env.example` template
  - [X] `TG_API_ID`, `TG_API_HASH` (Telegram)
  - [ ] `BOT_TOKEN` (Telegram BotFather)
  - [ ] `HF_TOKEN` (Hugging Face, optional)

---

## ğŸ“¥ Phase 1 â€” Data Collection

- [X] Use `pull_telegram.py` to fetch cloud messages
  - [X] Limit message scope (DMs, selected groups)
- [X] Validate message quality: fields present (text, timestamp, sender etc.)
- [X] Store raw data in `data/raw/messages.json`

---

## ğŸ› Phase 2 â€” Data Cleaning & Anonymization

- [ ] Implement `build_dataset.py`:
  - [ ] Replace other usersâ€™ identifiers with pseudonyms (`user_XXXX`)
  - [ ] Remove or mask:
    - PII (phone, email)
    - Sensitive URLs
- [ ] Filter messages:
  - [ ] Remove media-only messages
  - [ ] Optional: Keep emoji-only replies if stylistic
- [ ] Re-save cleaned dataset: `data/raw/messages_clean.json`

---

## ğŸ§© Phase 3 â€” Dataset Construction for SFT

- [ ] Implement windowed pairing (context â†’ your reply)
  - [ ] Choose window size (default: 6â€“10 exchanges)
- [ ] Add system prompt indicating style + persona
- [ ] Split into:
  - [ ] train (80%)
  - [ ] val (10%)
  - [ ] test (10%) by thread ID
- [ ] Save final formatted dataset â†’ `data/processed/sft_data.json`
- [ ] Create dataset stats (count, avg length, distribution)

---

## ğŸ§ª Phase 4 â€” Baseline Inference (No Training)

- [ ] Try out inference using:
  - [ ] Base model (e.g., Mistral-7B or Llama3-8B)
  - [ ] Very light prompt engineering using real examples
- [ ] Evaluate initial style closeness qualitatively

ğŸš€ This sets a baseline before fine-tuning.

---

## ğŸ§  Phase 5 â€” Fine-Tuning (QLoRA SFT)

- [ ] Configure `axolotl_config.yaml` with dataset paths + LoRA params
- [ ] Train using Axolotl:
  - [ ] Monitor loss + early stop if needed
- [ ] Save LoRA adapters â†’ `models/adapters/`
- [ ] Merge adapter with base model for runtime
- [ ] Validate on hold-out samples:
  - Did tone/style match yours?
  - Any hallucinations or inappropriate replies?

---

## ğŸ” Phase 6 â€” Retrieval Augmentation

- [ ] Build vector store (`FAISS`) in `data/embeddings/`
- [ ] Index:
  - Past messages
  - Persona anchors (example replies)
- [ ] Implement retrieval ranking in `retrieval_pipeline.py`:
  - [ ] Top-k similarity search (`k=3â€“5`)
  - [ ] Prompt template includes recent chat history + retrieved examples
- [ ] Integration test â†’ drastically improves realism

---

## ğŸ¤– Phase 7 â€” Telegram Bot Deployment

- [ ] Implement `/start` + message handler (`telegram_bot.py`)
- [ ] Connect to local inference server (vLLM/Ollama)
- [ ] Track conversational memory per user
- [ ] Add safety guardrails:
  - [ ] Block or warn on secrets
  - [ ] Max tokens/time limits
- [ ] Beta test with friends

---

## ğŸ¯ Phase 8 â€” Evaluation

- [ ] A/B test: real answer vs bot answer (shuffled)
- [ ] Style classifier:
  - âœ… classify: â€œAnselm vs. Generic AIâ€
- [ ] Track metrics:
  - Accuracy
  - Response quality (manual labels)
  - Latency

---

## âš™ï¸ Phase 9 â€” Preference Fine-Tuning (DPO/KTO)

- [ ] Activate preference feedback UI:
  - âœ… â€œYes, Iâ€™d respond like thatâ€
  - âŒ â€œNot like meâ€
- [ ] Collect dataset of preference pairs or ratings
- [ ] Implement DPO pass (`run_dpo.py`)
  - [ ] Re-fine-tune model using reward-aligned preference data
- [ ] Deploy updated bot
- [ ] Continuous updates as new data arrives

---

## ğŸ” Phase 10 â€” Privacy / Governance

- [ ] `/forget` command to delete conversation memory for any user
- [ ] Clearly disclose bot is an AI persona
- [ ] Store raw message exports locally only
- [ ] (Optional) Publish **LoRA adapter only** for safety

---

## ğŸ›£ï¸ Future Ideas

- [ ] Online RL with bounded exploration
- [ ] Emotion-aware responses
- [ ] Voice clone integration (TTS + style conditioning)
- [ ] Multi-persona mode (e.g. â€œProfessional Anselmâ€, â€œCasual Anselmâ€)
- [ ] Web UI dashboard to manage training loops & memory store

---

### âœ… Progress Summary Table

| Phase | Status | Notes |
|-------|--------|------|
| 0 â€” Env Setup | â¬œ | |
| 1 â€” Data Pull | â¬œ | |
| 2 â€” Cleaning | â¬œ | |
| 3 â€” SFT Dataset | â¬œ | |
| 4 â€” Baseline Inference | â¬œ | |
| 5 â€” QLoRA SFT | â¬œ | |
| 6 â€” Retrieval | â¬œ | |
| 7 â€” Telegram Bot | â¬œ | |
| 8 â€” Evaluation | â¬œ | |
| 9 â€” RL (DPO) | â¬œ | |
| 10 â€” Privacy | â¬œ | |
