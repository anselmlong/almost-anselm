{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2a3d4d",
   "metadata": {},
   "source": [
    "# Anselm — QLoRA fine-tuning with Axolotl (Colab-friendly)\n",
    "\n",
    "This notebook performs QLoRA fine-tuning of a Mistral-7B-Instruct base model using Axolotl. It:\n",
    "- Installs dependencies (PyTorch + Axolotl + bitsandbytes + peft)\n",
    "- Mounts Google Drive (optional) and prepares `data/processed/`\n",
    "- Writes `configs/anselm_qlora.yaml` with QLoRA + LoRA settings\n",
    "- Runs training via `accelerate launch -m axolotl.cli.train configs/anselm_qlora.yaml`\n",
    "- Shows TensorBoard for monitoring and includes a GPU monitor cell\n",
    "- Demonstrates loading the LoRA adapter for inference\n",
    "\n",
    "Run cells top-to-bottom. Select a GPU runtime in Colab (Runtime → Change runtime type → GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ebd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment check: GPU, Python, and basic info\n",
    "import os, sys, subprocess\n",
    "print('Python', sys.version)\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi','--query-gpu=name,memory.total --format=csv,noheader,nounits'], text=True)\n",
    "    print('GPU info:', gpu_info)\n",
    "except Exception as e:\n",
    "    print('nvidia-smi not available or no GPU: ', e)\n",
    "\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Install dependencies (Colab-friendly). Run once.\n",
    "# Adjust torch wheel index if you need a different CUDA version.\n",
    "print('Installing dependencies (may take several minutes)')\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q 'torch' torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q accelerate axolotl transformers bitsandbytes peft tensorboard sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8538a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Install completed. Verifying key packages...')\n",
    "import importlib\n",
    "for pkg in ('torch','accelerate','axolotl','transformers','bitsandbytes','peft','tensorboard'):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(pkg, 'version', getattr(m, '__version__', 'unknown'))\n",
    "    except Exception as e:\n",
    "        print(pkg, 'import error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Mount Google Drive if running on Colab (optional). If not on Colab, skip and ensure your data exists under data/processed/\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    print('Running on Colab — mounting Drive to /content/drive')\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    print('Not running on Colab or google.colab not available. If running locally, ensure your dataset is at data/processed/ or upload it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2becbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Prepare directories and copy dataset from Drive if present\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "content_data = Path('/content/data/processed')\n",
    "content_data.mkdir(parents=True, exist_ok=True)\n",
    "print('Ensured', content_data)\n",
    "drive_path = Path('/content/drive/MyDrive/almost-anselm/data/processed')\n",
    "if drive_path.exists():\n",
    "    print('Found dataset in Drive — copying to', content_data)\n",
    "    for p in drive_path.glob('*'):\n",
    "        dst = content_data / p.name\n",
    "        if dst.exists():\n",
    "            print('Skipping existing', dst)\n",
    "            continue\n",
    "        if p.is_dir():\n",
    "            shutil.copytree(p, dst)\n",
    "        else:\n",
    "            shutil.copy2(p, dst)\n",
    "    print('Copy complete')\n",
    "else:\n",
    "    print('Drive dataset not found at', drive_path, 'Make sure your train/val/test files are in', content_data)\n",
    "print('Current data/processed contents:')\n",
    "print(list(content_data.glob('*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae843082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Write the Axolotl config file for QLoRA: configs/anselm_qlora.yaml\n",
    "from pathlib import Path\n",
    "cfg_dir = Path('configs')\n",
    "cfg_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg_path = cfg_dir / 'anselm_qlora.yaml'\n",
    "cfg_text = '''\n",
    "base_model: mistral-7b-instruct-v0.3\n",
    "model_type: mistral\n",
    "load_in_4bit: true\n",
    "trust_remote_code: true\n",
    "torch_dtype: float16\n",
    "dataset:\n",
    "  train: data/processed/sft_train.json\n",
    "  val: data/processed/sft_val.json\n",
    "  test: data/processed/sft_test.json\n",
    "dataset_format: chatml\n",
    "tokenizer:\n",
    "  add_eos_token: true\n",
    "  add_bos_token: true\n",
    "sequence:\n",
    "  max_length: 2048\n",
    "  sample_packing: true\n",
    "training:\n",
    "  output_dir: models/almost-anselm-lora/\n",
    "  num_epochs: 1\n",
    "  per_device_train_batch_size: 1\n",
    "  gradient_accumulation_steps: 16\n",
    "  learning_rate: 1e-4\n",
    "  lr_scheduler: cosine\n",
    "  warmup_steps: 100\n",
    "  logging_steps: 50\n",
    "  eval_steps: 1000\n",
    "  save_steps: 1000\n",
    "  save_total_limit: 3\n",
    "  bf16: false\n",
    "  fp16: true\n",
    "  gradient_checkpointing: true\n",
    "  optim: paged_adamw_32bit\n",
    "lora:\n",
    "  enabled: true\n",
    "  r: 16\n",
    "  lora_alpha: 32\n",
    "  lora_dropout: 0.05\n",
    "  target_modules:\n",
    "    - q_proj\n",
    "    - k_proj\n",
    "    - v_proj\n",
    "    - o_proj\n",
    "'''\n",
    "cfg_path.write_text(cfg_text)\n",
    "print('Wrote', cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e079cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Print the config to verify\n",
    "print(open('configs/anselm_qlora.yaml','r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4ba37",
   "metadata": {},
   "source": [
    "## 7) Run training (Accelerate + Axolotl)\n",
    "\n",
    "The command below runs Axolotl training using `accelerate`. It writes stdout/stderr to `logs/axolotl_train.log` so you can inspect progress after starting. On Colab, running interactively will stream logs into the notebook output. If you prefer to run in background, use the `nohup` example provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba022fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory\n",
    "from pathlib import Path\n",
    "Path('logs').mkdir(exist_ok=True)\n",
    "print('When ready, run (uncomment) the accelerate command in this cell to start training.')\n",
    "# Example interactive run:\n",
    "# !accelerate launch -m axolotl.cli.train configs/anselm_qlora.yaml 2>&1 | tee logs/axolotl_train.log\n",
    "# Example background run:\n",
    "# !nohup accelerate launch -m axolotl.cli.train configs/anselm_qlora.yaml > logs/axolotl_train.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07885d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) TensorBoard — visualize training metrics (run after logs exist)\n",
    "try:\n",
    "    get_ipython().run_line_magic('load_ext','tensorboard')\n",
    "except Exception:\n",
    "    pass\n",
    "logdirs = ['runs','logs','models/almost-anselm-lora/runs']\n",
    "from pathlib import Path\n",
    "existing = [p for p in logdirs if Path(p).exists()]\n",
    "if not existing:\n",
    "    print('No TensorBoard logs found yet in', logdirs)\n",
    "else:\n",
    "    print('Starting TensorBoard for', existing[0])\n",
    "    get_ipython().run_line_magic('tensorboard', f'--logdir {existing[0]} --host 0.0.0.0 --port 6006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Tailing logs & simple GPU monitor (run interactively; interrupt to stop)\n",
    "import time, subprocess\n",
    "from pathlib import Path\n",
    "log_path = Path('logs/axolotl_train.log')\n",
    "print('Tailing', log_path)\n",
    "try:\n",
    "    while True:\n",
    "        if log_path.exists():\n",
    "            tail = subprocess.run(['tail','-n','20', str(log_path)], capture_output=True, text=True)\n",
    "            print('--- last 20 lines of log ---')\n",
    "            print(tail.stdout)\n",
    "        else:\n",
    "            print('Log not found yet at', log_path)\n",
    "        try:\n",
    "            g = subprocess.check_output(['nvidia-smi','--query-gpu=index,name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits'], text=True)\n",
    "            print('GPU:')\n",
    "            print(g)\n",
    "        except Exception:\n",
    "            print('nvidia-smi not available')\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print('Stopped tailing/log monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72018908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) After training: inspect or copy adapter output into models/almost-anselm-lora/\n",
    "from pathlib import Path\n",
    "outdir = Path('models/almost-anselm-lora')\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "print('Contents of models/ (top-level):')\n",
    "print(list(Path('models').glob('*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a13f3",
   "metadata": {},
   "source": [
    "## 11) Inference: Load base model (4-bit) and the trained LoRA adapter\n",
    "\n",
    "This cell uses `transformers`, `bitsandbytes`, and `peft` to load the base model with 4-bit quantization and then load the LoRA adapter from `models/almost-anselm-lora/`. It runs a few sample prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f21986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11a) Inference loader and generator\n",
    "import os\n",
    "from pathlib import Path\n",
    "adapter_path = Path('models/almost-anselm-lora')\n",
    "if not adapter_path.exists() or not any(adapter_path.iterdir()):\n",
    "    print('Adapter dir appears empty; ensure you copied adapter checkpoints to', adapter_path)\n",
    "else:\n",
    "    print('Adapter path:', adapter_path)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    print('Missing packages for inference:', e)\n",
    "    raise\n",
    "MODEL_NAME = 'mistral-7b-instruct-v0.3'\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print('Loading 4-bit model (this may take a while)...')\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit=True, device_map='auto', trust_remote_code=True)\n",
    "print('Base model loaded')\n",
    "print('Attempting to load LoRA adapter from', adapter_path)\n",
    "model = PeftModel.from_pretrained(model, adapter_path, device_map='auto')\n",
    "print('Adapter loaded successfully')\n",
    "def generate(prompt, max_new_tokens=128, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "prompts = [\n",
    "    'Write a short reflective reply to: I had a rough day and could use some advice.',\n",
    "    'Explain briefly why patience matters in conversation.'\n",
    "]\n",
    "for p in prompts:\n",
    "    print('\n",
    "PROMPT:', p)\n",
    "    print('REPLY:', generate(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cca86",
   "metadata": {},
   "source": [
    "## 12) CLI command (run outside notebook)\n",
    "\n",
    "Use this command from the repository root (ensure `accelerate` config is set up):\n",
    "```\n",
    "accelerate launch -m axolotl.cli.train configs/anselm_qlora.yaml\n",
    "```\n",
    "You can redirect logs to a file:\n",
    "```\n",
    "accelerate launch -m axolotl.cli.train configs/anselm_qlora.yaml 2>&1 | tee logs/axolotl_train.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2cde88",
   "metadata": {},
   "source": [
    "## Troubleshooting & Notes\n",
    "\n",
    "- CUDA / Torch mismatch: If `torch` fails to import or GPU isn't available, install the matching torch wheel for your CUDA version. In Colab, CUDA 11.8 wheels usually work.\n",
    "- OOM: If you run out of memory, reduce `per_device_train_batch_size` or increase gradient accumulation. You can also use `load_in_4bit: true` (already enabled) and `gradient_checkpointing: true`.\n",
    "- `trust_remote_code: true` is convenient but review code for untrusted checkpoints.\n",
    "- If LoRA adapter doesn't load, inspect the saved checkpoint folder and ensure the adapter files (adapter_config.json, adapter_model.bin/weights) exist. The `peft` loader expects a compatible layout.\n",
    "- To reproduce training parameters outside Colab, run the `accelerate launch` command above after configuring `accelerate config`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
